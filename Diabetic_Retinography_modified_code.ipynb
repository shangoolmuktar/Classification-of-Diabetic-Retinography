{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Features Shape: (50, 18)\n",
      "Train Labels Length: 50\n",
      "Test Features Shape: (50, 18)\n",
      "Test Labels Length: 50\n"
     ]
    }
   ],
   "source": [
    "#LBP to Green Channel of Images\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# LBP parameters\n",
    "radius = 2\n",
    "n_points = 8 * radius\n",
    "METHOD = 'uniform'\n",
    "\n",
    "train_dir =r'F:\\for Drishti dataset\\DRISHTI-gs_Nayab\\Training-20211018T055246Z-001\\Training\\Images'\n",
    "test_dir = r'F:\\for Drishti dataset\\DRISHTI-gs_Nayab\\Test-20211018T060000Z-001\\Test\\Images'\n",
    "\n",
    "\n",
    "\n",
    "train_features = np.empty((0, n_points + 2))\n",
    "train_labels = []\n",
    "test_features = np.empty((0, n_points + 2))\n",
    "test_labels = []\n",
    "\n",
    "for root, dirs, files in os.walk(train_dir):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        # Load the image\n",
    "            img = cv2.imread(os.path.join(root, filename))\n",
    "            #green_channel = img[:, :, 1]  # extract green channel\n",
    "            #gray = cv2.resize(green_channel, (512, 512), interpolation=cv2.INTER_LINEAR)\n",
    "            b, g, r = cv2.split(img)\n",
    "            gray = g\n",
    "            img = cv2.resize(gray, (512, 512),interpolation = cv2.INTER_LINEAR)\n",
    " \n",
    "            # Calculate LBP image\n",
    "            lbp = local_binary_pattern(img, n_points, radius, METHOD)\n",
    "\n",
    "            # Calculate histogram of LBP image\n",
    "            hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "\n",
    "            # Normalize histogram\n",
    "            hist = hist.astype(\"float\")\n",
    "            hist /= (hist.sum() + 1e-7)\n",
    "\n",
    "            # Append histogram to array\n",
    "            train_features = np.vstack((train_features, hist))\n",
    "            X_label = os.path.basename(root)\n",
    "            train_labels.append(X_label)\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(test_dir):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        # Load the image\n",
    "            img = cv2.imread(os.path.join(root, filename))\n",
    "           # green_channel = img[:, :, 1]  # extract green channel\n",
    "           # gray = cv2.resize(green_channel, (512, 512), interpolation=cv2.INTER_LINEAR)\n",
    "            b, g, r = cv2.split(img)\n",
    "            gray = g\n",
    "            img = cv2.resize(gray, (512, 512),interpolation = cv2.INTER_LINEAR)\n",
    " \n",
    "            # Calculate LBP image\n",
    "            lbp = local_binary_pattern(img, n_points, radius, METHOD)\n",
    "\n",
    "            # Calculate histogram of LBP image\n",
    "            hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "\n",
    "            # Normalize histogram\n",
    "            hist = hist.astype(\"float\")\n",
    "            hist /= (hist.sum() + 1e-7)\n",
    "\n",
    "            \n",
    "            test_features = np.vstack((test_features, hist))\n",
    "            Y_label = os.path.basename(root)\n",
    "            test_labels.append(Y_label)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "np.savetxt('train_features.csv', train_features, delimiter=',')\n",
    "np.savetxt('train_labels.csv', train_labels, delimiter=',', fmt='%s')\n",
    "np.savetxt('test_features.csv', test_features, delimiter=',')\n",
    "np.savetxt('test_labels.csv', test_labels, delimiter=',', fmt='%s')\n",
    "\n",
    "\n",
    "\n",
    "print('Train Features Shape:', train_features.shape)\n",
    "print('Train Labels Length:', len(train_labels))\n",
    "print('Test Features Shape:', test_features.shape)\n",
    "print('Test Labels Length:', len(test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "train_features = np.loadtxt('train_features.csv', delimiter=',')\n",
    "train_labels = np.loadtxt('train_labels.csv', delimiter=',', dtype=str)\n",
    "test_features = np.loadtxt('test_features.csv', delimiter=',')\n",
    "test_labels = np.loadtxt('test_labels.csv', delimiter=',', dtype=str)\n",
    "#classifier= KNeighborsClassifier(n_neighbors=1, metric='minkowski', p=2 ) Accuracy: 46.0 \n",
    "#classifier= KNeighborsClassifier(n_neighbors=3, metric='minkowski', p=2 )  #Accuracy: 54.0\n",
    "#classifier= KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2 ) #Accuracy: 54.0# \n",
    "\n",
    "#classifier= KNeighborsClassifier(n_neighbors=29, metric='minkowski', p=2 )  # Accuracy 70.0\n",
    "classifier= KNeighborsClassifier(n_neighbors=35, metric='minkowski', p=2 )  \n",
    "\n",
    "classifier.fit(train_features, train_labels)  \n",
    "\n",
    "y_pred = classifier.predict(test_features)\n",
    "\n",
    "# Evaluate the accuracy of the SVM classifier\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "\n",
    "# Print the accuracy of the SVM classifier\n",
    "print(\"Accuracy:\", accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.0\n"
     ]
    }
   ],
   "source": [
    "#Decision tree\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.metrics import accuracy_score\n",
    "train_features = np.loadtxt('train_features.csv', delimiter=',')\n",
    "train_labels = np.loadtxt('train_labels.csv', delimiter=',', dtype=str)\n",
    "test_features = np.loadtxt('test_features.csv', delimiter=',')\n",
    "test_labels = np.loadtxt('test_labels.csv', delimiter=',', dtype=str)\n",
    "\n",
    "classifier= DecisionTreeClassifier(criterion='entropy', random_state=42,max_depth=0.25)  \n",
    "\n",
    "\n",
    "classifier.fit(train_features,train_labels)\n",
    "y_pred=classifier.predict(test_features)\n",
    "accuracy=accuracy_score(test_labels,y_pred)\n",
    "print(\"Accuracy:\", accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.0\n"
     ]
    }
   ],
   "source": [
    "#Random FOREST\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_features = np.loadtxt('train_features.csv', delimiter=',')\n",
    "train_labels = np.loadtxt('train_labels.csv', delimiter=',', dtype=str)\n",
    "test_features = np.loadtxt('test_features.csv', delimiter=',')\n",
    "test_labels = np.loadtxt('test_labels.csv', delimiter=',', dtype=str)\n",
    "\n",
    "classifier= RandomForestClassifier(n_estimators=14, criterion=\"entropy\",random_state=5)  \n",
    "classifier.fit(train_features, train_labels)\n",
    "y_pred=classifier.predict(test_features)\n",
    "accuracy=accuracy_score(test_labels,y_pred)\n",
    "print(\"Accuracy:\", accuracy*100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.0\n"
     ]
    }
   ],
   "source": [
    "#Bagging\n",
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_features = np.loadtxt('train_features.csv', delimiter=',')\n",
    "train_labels = np.loadtxt('train_labels.csv', delimiter=',', dtype=str)\n",
    "test_features = np.loadtxt('test_features.csv', delimiter=',')\n",
    "test_labels = np.loadtxt('test_labels.csv', delimiter=',', dtype=str)\n",
    "classifier= BaggingClassifier(n_estimators=10,  random_state=5)  \n",
    "classifier.fit(train_features, train_labels)\n",
    "y_pred=classifier.predict(test_features)\n",
    "accuracy=accuracy_score(test_labels,y_pred)\n",
    "print(\"Accuracy:\", accuracy*100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.0\n",
      "Confusion Matrix\n",
      "[[35  2]\n",
      " [11  2]]\n",
      "F1 Score: 0.685272856130404\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Load train and test feature matrices and label lists from CSV files\n",
    "train_features = np.loadtxt('train_features.csv', delimiter=',')\n",
    "train_labels = np.loadtxt('train_labels.csv', delimiter=',', dtype=str)\n",
    "test_features = np.loadtxt('test_features.csv', delimiter=',')\n",
    "test_labels = np.loadtxt('test_labels.csv', delimiter=',', dtype=str)\n",
    "\n",
    "# Create an SVM classifier with a linear kernel\n",
    "#clf = svm.SVC(kernel='linear')\n",
    "clf=svm.SVC(kernel='rbf', C=30, gamma=50) # best parameters for drishti\n",
    "#clf = svm.SVC(kernel='poly', C=10, gamma=100)\n",
    "# Train the SVM classifier on the training set\n",
    "clf.fit(train_features, train_labels)\n",
    "\n",
    "# Predict the labels of the test set using the trained SVM classifier\n",
    "y_pred = clf.predict(test_features)\n",
    "\n",
    "# Evaluate the accuracy of the SVM classifier\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "\n",
    "# Print the accuracy of the SVM classifier\n",
    "print(\"Accuracy:\", accuracy*100)\n",
    "\n",
    "confusion_mat = confusion_matrix(test_labels,y_pred)\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_mat)\n",
    "f1 = f1_score(test_labels, y_pred, average='weighted')\n",
    "\n",
    "# Print the F1 score\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Features Mean: [0.12704089 0.12195577 0.06651076 0.07476941 0.07279762 0.1267124\n",
      " 0.19846362 0.21174953]\n",
      "Train Features Standard Deviation: [0.00774933 0.00292954 0.00845762 0.01734573 0.01039835 0.00432543\n",
      " 0.01622077 0.01196198]\n",
      "Merge Features Shape: (100, 8)\n",
      "merge Labels Length: 100\n"
     ]
    }
   ],
   "source": [
    "#merging\n",
    "merge_dir =r'F:\\for Drishti dataset\\Merged'\n",
    "# Set LBP parameters\n",
    "radius = 3\n",
    "n_points = 2 * radius\n",
    "method = 'uniform'\n",
    "n_bins = n_points + 2\n",
    "\n",
    "merge_features = np.empty((0, n_points + 2))\n",
    "merge_labels = []\n",
    "\n",
    "# Loop through train images and extract LBP features and labels\n",
    "for root, dirs, files in os.walk(merge_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg') or file.endswith('.png'):\n",
    "             # Read image and extract green channel\n",
    "            img = cv2.imread(os.path.join(root, file))\n",
    "            b, g, r = cv2.split(img)\n",
    "            gray = g\n",
    "                      \n",
    "            # Compute LBP features and histogram\n",
    "            lbp = local_binary_pattern(gray, n_points, radius, method)\n",
    "            hist, _ = np.histogram(lbp, bins=np.arange(n_bins+1), density=True)\n",
    "            # Append features and label to train set\n",
    "            merge_features = np.vstack((merge_features, hist))\n",
    "            X_label = os.path.basename(root)\n",
    "            merge_labels.append(X_label)\n",
    "\n",
    "# Save feature and label matrices into CSV files\n",
    "np.savetxt('merge_features.csv', merge_features, delimiter=',')\n",
    "np.savetxt('merge_labels.csv', merge_labels, delimiter=',', fmt='%s')\n",
    "mean = np.mean(merge_features, axis=0)\n",
    "std = np.std(merge_features, axis=0)\n",
    "print('Train Features Mean:', mean)\n",
    "print('Train Features Standard Deviation:', std)\n",
    "print('Merge Features Shape:', merge_features.shape)\n",
    "print('merge Labels Length:', len(merge_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge_labels [[1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "merge_features = np.loadtxt('merge_features.csv', delimiter=',')\n",
    "merge_labels = np.loadtxt('merge_features.csv', delimiter=',', dtype=str)\n",
    "merge_labels = np.where(merge_labels == 'glaucoma', 0, 1)\n",
    "print('merge_labels', merge_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Average CV Score: 1.0\n",
      "Number of CV Scores used in Average: 10\n",
      "Testing Times: [0.0, 0.008000612258911133, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "\n",
    "\n",
    "# Create a classifier\n",
    "\n",
    "clf= DecisionTreeClassifier(criterion='entropy', random_state=42,max_depth=0.25) \n",
    "#clf= BaggingClassifier(n_estimators=10,  random_state=5)\n",
    "#clf = svm.SVC(kernel='linear', C=0.9, gamma=0.5, probability=True)#0.69\n",
    "#clf=KNeighborsClassifier(n_neighbors=27, metric='minkowski', p=2 ) #0.62\n",
    "\n",
    "#clf= RandomForestClassifier(n_estimators=14, criterion=\"entropy\",random_state=5)  \n",
    "\n",
    "# Create a KFold object\n",
    "k_folds = KFold(n_splits=10)\n",
    "\n",
    "# List to store cross-validation scores\n",
    "scores = []\n",
    "testing_times = []\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in k_folds.split(merge_features):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = merge_features[train_index], merge_features[test_index]\n",
    "    y_train, y_test = merge_labels[train_index], merge_labels[test_index]\n",
    "\n",
    "    # Train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "# Predict labels for test instances and measure testing time\n",
    "    start_time = time.time()\n",
    "    y_pred = clf.predict(X_test)\n",
    "    end_time = time.time()\n",
    "    testing_time = end_time - start_time\n",
    "    testing_times.append(testing_time)\n",
    "    # Evaluate the classifier\n",
    "    score = clf.score(X_test, y_test)\n",
    "    scores.append(score)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross Validation Scores:\", scores)\n",
    "print(\"Average CV Score:\", np.mean(scores))\n",
    "print(\"Number of CV Scores used in Average:\", len(scores))\n",
    "# Print the testing times\n",
    "print(\"Testing Times:\", testing_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Normalize train and test feature matrices\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(merge_features)\n",
    "np.random.seed(42)\n",
    "print(merge_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: ['glaucoma']\n",
      "Is this right?  (y/n)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.feature import local_binary_pattern\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "#from sklearn.isotonic import IsotonicRegression\n",
    "# Define LBP parameters\n",
    "radius = 2\n",
    "n_points = 8 * radius\n",
    "METHOD = 'uniform'\n",
    "train_csv_file = 'train_features.csv'\n",
    "train_label_csv = 'train_labels.csv'\n",
    "test_features = np.empty((0, n_points + 2))\n",
    "# Load train and test feature matrices and label lists from CSV files\n",
    "train_features = np.loadtxt(train_csv_file, delimiter=',')\n",
    "train_labels = np.loadtxt(train_label_csv, delimiter=',', dtype=str)\n",
    "\n",
    "# Create an SVM classifier with a linear kernel\n",
    "\n",
    "clf=svm.SVC(kernel='rbf', C=30, gamma=50) # best parameters for drishti\n",
    "#clf = svm.SVC(kernel='poly', C=131.1, gamma=150)\n",
    "# Train the SVM classifier on the training set\n",
    "clf.fit(train_features, train_labels)\n",
    "\n",
    "url=input(\"enter url\")\n",
    "img=cv2.imread(url)\n",
    "b, g, r = cv2.split(img)\n",
    "gray = g\n",
    "img = cv2.resize(gray, (512, 512),interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "# Calculate LBP image\n",
    "lbp = local_binary_pattern(img, n_points, radius, METHOD)\n",
    "# Calculate histogram of LBP image\n",
    "hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "# Normalize histogram\n",
    "hist = hist.astype(\"float\")\n",
    "hist /= (hist.sum() + 1e-7)\n",
    " # Append histogram to array\n",
    "test_features = np.vstack((test_features, hist))\n",
    "# predict label of test image using trained SVM model\n",
    "predicted_label = clf.predict(test_features)\n",
    "print('Predicted label:', predicted_label)\n",
    "\n",
    "Categories =  np.unique(train_labels)\n",
    "\n",
    "print(f'Is this right?  (y/n)')\n",
    "while(True):\n",
    "  b=input()\n",
    "  if(b==\"y\" or b==\"n\"):\n",
    "    break\n",
    "  print(\"please enter either y or n\")\n",
    "\n",
    "if(b=='n'):\n",
    "  print(\"What is the image?\")\n",
    "  for i in range(len(Categories)):\n",
    "    print(f\"Enter {i} for {Categories[i]}\")\n",
    "  k=int(input())\n",
    "  while(k<0 or k>=len(Categories)):\n",
    "    print(f\"Please enter a valid number between 0-{len(Categories)-1}\")\n",
    "    k=int(input())\n",
    "  print(\"Please wait for a while for the model to learn from this image :)\")\n",
    "\n",
    "  # Print the shape of the feature matrix and length of label list for train and test sets\n",
    "  print('Train Features Shape:', train_features.shape)\n",
    "  print('Train Labels Length:', len(train_labels))\n",
    "  train_features = np.vstack((train_features, test_features))\n",
    "  train_labels = np.append(train_labels,Categories[k])\n",
    "  np.savetxt(train_csv_file, train_features, delimiter=',')\n",
    "  np.savetxt(train_label_csv, train_labels, delimiter=',', fmt='%s')\n",
    "   # Print the shape of the feature matrix and length of label list for train and test sets\n",
    "  print('Train Features Shape-2:', train_features.shape)\n",
    "  print('Train Labels Length-2:', len(train_labels))\n",
    "  \n",
    "  # Train the SVM classifier on the training set\n",
    "  clf.fit(train_features, train_labels)\n",
    "\n",
    "  # predict label of test image using trained SVM model\n",
    "  predicted_label = clf.predict(test_features)\n",
    "  print('Predicted label:', predicted_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6c3fe0f35ac205adeee3d33d4d27f45ea8735944f6bcc7cc4d72c9672a115a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
